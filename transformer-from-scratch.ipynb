{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9676451,"sourceType":"datasetVersion","datasetId":5914021}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/guitaristboy/transformer-from-scratch?scriptVersionId=209073102\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-22T19:23:37.38196Z","iopub.execute_input":"2024-11-22T19:23:37.382344Z","iopub.status.idle":"2024-11-22T19:23:37.394251Z","shell.execute_reply.started":"2024-11-22T19:23:37.382309Z","shell.execute_reply":"2024-11-22T19:23:37.393088Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/input-txt/input.txt\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T19:23:39.164307Z","iopub.execute_input":"2024-11-22T19:23:39.16472Z","iopub.status.idle":"2024-11-22T19:23:39.170106Z","shell.execute_reply.started":"2024-11-22T19:23:39.164683Z","shell.execute_reply":"2024-11-22T19:23:39.168703Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# hyperparameter\n# how many independent sequences will we process in parallel?\nbatch_size = 32\n# what is the maximum context ellgnth for predeictions?\nblock_size = 8 \nmax_iters = 100000\neval_interval = 500\nlearning_rate = 1e-2\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\neval_iters = 200\nn_embd = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T19:23:39.99706Z","iopub.execute_input":"2024-11-22T19:23:39.997462Z","iopub.status.idle":"2024-11-22T19:23:40.0035Z","shell.execute_reply.started":"2024-11-22T19:23:39.997423Z","shell.execute_reply":"2024-11-22T19:23:40.002228Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"torch.manual_seed(1337)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T19:23:40.899591Z","iopub.execute_input":"2024-11-22T19:23:40.900513Z","iopub.status.idle":"2024-11-22T19:23:40.909933Z","shell.execute_reply.started":"2024-11-22T19:23:40.900478Z","shell.execute_reply":"2024-11-22T19:23:40.908947Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f855965ae90>"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"with open('/kaggle/input/input-txt/input.txt','r',encoding='utf-8') as  f:\n    text = f.read()","metadata":{"execution":{"iopub.status.busy":"2024-11-22T18:17:40.082971Z","iopub.execute_input":"2024-11-22T18:17:40.083317Z","iopub.status.idle":"2024-11-22T18:17:40.138505Z","shell.execute_reply.started":"2024-11-22T18:17:40.083281Z","shell.execute_reply":"2024-11-22T18:17:40.137251Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print('Length of the dataset in character is ' , len(text))","metadata":{"execution":{"iopub.status.busy":"2024-11-22T18:17:40.14002Z","iopub.execute_input":"2024-11-22T18:17:40.140366Z","iopub.status.idle":"2024-11-22T18:17:40.146195Z","shell.execute_reply.started":"2024-11-22T18:17:40.140334Z","shell.execute_reply":"2024-11-22T18:17:40.144912Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Length of the dataset in character is  1115394\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# here are the unique character that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-11-22T18:17:40.147829Z","iopub.execute_input":"2024-11-22T18:17:40.148276Z","iopub.status.idle":"2024-11-22T18:17:40.174447Z","shell.execute_reply.started":"2024-11-22T18:17:40.148239Z","shell.execute_reply":"2024-11-22T18:17:40.173052Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi = {ch:i for i , ch in enumerate(chars)}\nitos = {i:ch for i , ch in enumerate(chars)}\n# encoder: take string , output a list of integers\nencode = lambda s : [stoi[c] for c in s] \n# decoder: takes input of integers , outputs string\ndecode = lambda l : ''.join([itos[i] for i in l])\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))","metadata":{"execution":{"iopub.status.busy":"2024-11-22T18:17:40.176023Z","iopub.execute_input":"2024-11-22T18:17:40.176371Z","iopub.status.idle":"2024-11-22T18:17:40.191944Z","shell.execute_reply.started":"2024-11-22T18:17:40.176339Z","shell.execute_reply":"2024-11-22T18:17:40.19064Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\ndata = torch.tensor(encode(text),dtype = torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000])","metadata":{"execution":{"iopub.status.busy":"2024-11-22T18:17:40.193619Z","iopub.execute_input":"2024-11-22T18:17:40.194097Z","iopub.status.idle":"2024-11-22T18:17:40.457326Z","shell.execute_reply.started":"2024-11-22T18:17:40.194048Z","shell.execute_reply":"2024-11-22T18:17:40.455965Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# lets now split up the data into train and validation sets\nn = int(0.9 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"execution":{"iopub.status.busy":"2024-11-22T18:17:40.460075Z","iopub.execute_input":"2024-11-22T18:17:40.460388Z","iopub.status.idle":"2024-11-22T18:17:40.465969Z","shell.execute_reply.started":"2024-11-22T18:17:40.460358Z","shell.execute_reply":"2024-11-22T18:17:40.464813Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:17:40.467739Z","iopub.execute_input":"2024-11-22T18:17:40.468225Z","iopub.status.idle":"2024-11-22T18:17:40.485913Z","shell.execute_reply.started":"2024-11-22T18:17:40.46817Z","shell.execute_reply":"2024-11-22T18:17:40.484792Z"}},"outputs":[{"name":"stdout","text":"when input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split ==\"train\" else val_data\n    ix = torch.randint(len(data) - block_size , (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n\n    return x, y\n\nxb , yb = get_batch('train')\nprint('inputs :')\nprint(xb.shape)\nprint(xb)\nprint('targets : ')\n\nprint(yb.shape)\nprint(yb)\n\n\nprint('-------------')\n\n# batch dimension\nfor b in range(batch_size):\n    # time dimension\n    for t in range(block_size):\n        context = xb[b,:t+1]\n        target = yb[b,t]\n        print(f\"When input is {context.tolist()} the target: {target}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:17:59.590943Z","iopub.execute_input":"2024-11-22T18:17:59.591332Z","iopub.status.idle":"2024-11-22T18:17:59.614383Z","shell.execute_reply.started":"2024-11-22T18:17:59.591298Z","shell.execute_reply":"2024-11-22T18:17:59.612684Z"}},"outputs":[{"name":"stdout","text":"inputs :\ntorch.Size([32, 8])\ntensor([[52, 57,  8,  0, 21,  5, 50, 50],\n        [47, 58, 46,  1, 63, 53, 59,  6],\n        [ 1, 53, 40, 43, 63, 12,  0, 26],\n        [50, 42, 57, 58,  6,  0, 32, 46],\n        [ 1, 21,  1, 51, 39, 63,  7,  7],\n        [ 1, 59, 54,  1, 40, 63,  1, 19],\n        [43,  1, 43, 52, 53, 59, 45, 46],\n        [42, 10,  1, 61, 43,  1, 41, 39],\n        [20, 53, 61,  1, 44, 53, 52, 42],\n        [58, 43, 56, 51, 47, 52, 43, 57],\n        [43,  1, 53, 44,  1, 58, 56, 59],\n        [43, 52,  1, 44, 56, 53, 51,  1],\n        [11,  1, 47, 58,  1, 47, 57,  1],\n        [56, 47, 58,  1, 61, 43, 56, 43],\n        [47, 43, 58,  6,  1, 53, 52,  1],\n        [40, 56, 53, 58, 46, 43, 56,  1],\n        [59, 56,  1, 53, 44,  1, 58, 46],\n        [13, 52, 42,  1, 57, 39, 47, 42],\n        [45, 39, 56, 50, 39, 52, 42,  8],\n        [43, 39, 56, 57,  6,  0, 13, 52],\n        [ 8,  0,  0, 17, 16, 35, 13, 30],\n        [47, 57,  1, 58, 46, 43,  1, 61],\n        [ 1, 52, 47, 43, 41, 43,  1, 28],\n        [59, 54,  1, 58, 46, 43,  1, 63],\n        [47, 58,  1, 57, 43, 43, 51, 57],\n        [43, 50,  1, 51, 43,  1, 58, 53],\n        [39, 50, 50,  1, 42, 43, 57, 54],\n        [ 1, 54, 56, 39, 63,  1, 63, 53],\n        [43, 57, 58,  6,  1, 53, 44,  1],\n        [53,  6,  1, 61, 46, 43, 52,  1],\n        [ 1, 58, 56, 59, 43,  1, 44, 43],\n        [39, 58,  1, 21,  1, 57, 46, 39]])\ntargets : \ntorch.Size([32, 8])\ntensor([[57,  8,  0, 21,  5, 50, 50,  1],\n        [58, 46,  1, 63, 53, 59,  6,  1],\n        [53, 40, 43, 63, 12,  0, 26, 39],\n        [42, 57, 58,  6,  0, 32, 46, 43],\n        [21,  1, 51, 39, 63,  7,  7, 58],\n        [59, 54,  1, 40, 63,  1, 19, 53],\n        [ 1, 43, 52, 53, 59, 45, 46, 11],\n        [10,  1, 61, 43,  1, 41, 39, 52],\n        [53, 61,  1, 44, 53, 52, 42, 50],\n        [43, 56, 51, 47, 52, 43, 57,  0],\n        [ 1, 53, 44,  1, 58, 56, 59, 43],\n        [52,  1, 44, 56, 53, 51,  1, 39],\n        [ 1, 47, 58,  1, 47, 57,  1, 39],\n        [47, 58,  1, 61, 43, 56, 43,  1],\n        [43, 58,  6,  1, 53, 52,  1, 32],\n        [56, 53, 58, 46, 43, 56,  1, 47],\n        [56,  1, 53, 44,  1, 58, 46, 43],\n        [52, 42,  1, 57, 39, 47, 42,  1],\n        [39, 56, 50, 39, 52, 42,  8,  1],\n        [39, 56, 57,  6,  0, 13, 52, 42],\n        [ 0,  0, 17, 16, 35, 13, 30, 16],\n        [57,  1, 58, 46, 43,  1, 61, 47],\n        [52, 47, 43, 41, 43,  1, 28, 50],\n        [54,  1, 58, 46, 43,  1, 63, 43],\n        [58,  1, 57, 43, 43, 51, 57,  1],\n        [50,  1, 51, 43,  1, 58, 53,  1],\n        [50, 50,  1, 42, 43, 57, 54, 39],\n        [54, 56, 39, 63,  1, 63, 53, 59],\n        [57, 58,  6,  1, 53, 44,  1, 61],\n        [ 6,  1, 61, 46, 43, 52,  1, 58],\n        [58, 56, 59, 43,  1, 44, 43, 39],\n        [58,  1, 21,  1, 57, 46, 39, 50]])\n-------------\nWhen input is [52] the target: 57\nWhen input is [52, 57] the target: 8\nWhen input is [52, 57, 8] the target: 0\nWhen input is [52, 57, 8, 0] the target: 21\nWhen input is [52, 57, 8, 0, 21] the target: 5\nWhen input is [52, 57, 8, 0, 21, 5] the target: 50\nWhen input is [52, 57, 8, 0, 21, 5, 50] the target: 50\nWhen input is [52, 57, 8, 0, 21, 5, 50, 50] the target: 1\nWhen input is [47] the target: 58\nWhen input is [47, 58] the target: 46\nWhen input is [47, 58, 46] the target: 1\nWhen input is [47, 58, 46, 1] the target: 63\nWhen input is [47, 58, 46, 1, 63] the target: 53\nWhen input is [47, 58, 46, 1, 63, 53] the target: 59\nWhen input is [47, 58, 46, 1, 63, 53, 59] the target: 6\nWhen input is [47, 58, 46, 1, 63, 53, 59, 6] the target: 1\nWhen input is [1] the target: 53\nWhen input is [1, 53] the target: 40\nWhen input is [1, 53, 40] the target: 43\nWhen input is [1, 53, 40, 43] the target: 63\nWhen input is [1, 53, 40, 43, 63] the target: 12\nWhen input is [1, 53, 40, 43, 63, 12] the target: 0\nWhen input is [1, 53, 40, 43, 63, 12, 0] the target: 26\nWhen input is [1, 53, 40, 43, 63, 12, 0, 26] the target: 39\nWhen input is [50] the target: 42\nWhen input is [50, 42] the target: 57\nWhen input is [50, 42, 57] the target: 58\nWhen input is [50, 42, 57, 58] the target: 6\nWhen input is [50, 42, 57, 58, 6] the target: 0\nWhen input is [50, 42, 57, 58, 6, 0] the target: 32\nWhen input is [50, 42, 57, 58, 6, 0, 32] the target: 46\nWhen input is [50, 42, 57, 58, 6, 0, 32, 46] the target: 43\nWhen input is [1] the target: 21\nWhen input is [1, 21] the target: 1\nWhen input is [1, 21, 1] the target: 51\nWhen input is [1, 21, 1, 51] the target: 39\nWhen input is [1, 21, 1, 51, 39] the target: 63\nWhen input is [1, 21, 1, 51, 39, 63] the target: 7\nWhen input is [1, 21, 1, 51, 39, 63, 7] the target: 7\nWhen input is [1, 21, 1, 51, 39, 63, 7, 7] the target: 58\nWhen input is [1] the target: 59\nWhen input is [1, 59] the target: 54\nWhen input is [1, 59, 54] the target: 1\nWhen input is [1, 59, 54, 1] the target: 40\nWhen input is [1, 59, 54, 1, 40] the target: 63\nWhen input is [1, 59, 54, 1, 40, 63] the target: 1\nWhen input is [1, 59, 54, 1, 40, 63, 1] the target: 19\nWhen input is [1, 59, 54, 1, 40, 63, 1, 19] the target: 53\nWhen input is [43] the target: 1\nWhen input is [43, 1] the target: 43\nWhen input is [43, 1, 43] the target: 52\nWhen input is [43, 1, 43, 52] the target: 53\nWhen input is [43, 1, 43, 52, 53] the target: 59\nWhen input is [43, 1, 43, 52, 53, 59] the target: 45\nWhen input is [43, 1, 43, 52, 53, 59, 45] the target: 46\nWhen input is [43, 1, 43, 52, 53, 59, 45, 46] the target: 11\nWhen input is [42] the target: 10\nWhen input is [42, 10] the target: 1\nWhen input is [42, 10, 1] the target: 61\nWhen input is [42, 10, 1, 61] the target: 43\nWhen input is [42, 10, 1, 61, 43] the target: 1\nWhen input is [42, 10, 1, 61, 43, 1] the target: 41\nWhen input is [42, 10, 1, 61, 43, 1, 41] the target: 39\nWhen input is [42, 10, 1, 61, 43, 1, 41, 39] the target: 52\nWhen input is [20] the target: 53\nWhen input is [20, 53] the target: 61\nWhen input is [20, 53, 61] the target: 1\nWhen input is [20, 53, 61, 1] the target: 44\nWhen input is [20, 53, 61, 1, 44] the target: 53\nWhen input is [20, 53, 61, 1, 44, 53] the target: 52\nWhen input is [20, 53, 61, 1, 44, 53, 52] the target: 42\nWhen input is [20, 53, 61, 1, 44, 53, 52, 42] the target: 50\nWhen input is [58] the target: 43\nWhen input is [58, 43] the target: 56\nWhen input is [58, 43, 56] the target: 51\nWhen input is [58, 43, 56, 51] the target: 47\nWhen input is [58, 43, 56, 51, 47] the target: 52\nWhen input is [58, 43, 56, 51, 47, 52] the target: 43\nWhen input is [58, 43, 56, 51, 47, 52, 43] the target: 57\nWhen input is [58, 43, 56, 51, 47, 52, 43, 57] the target: 0\nWhen input is [43] the target: 1\nWhen input is [43, 1] the target: 53\nWhen input is [43, 1, 53] the target: 44\nWhen input is [43, 1, 53, 44] the target: 1\nWhen input is [43, 1, 53, 44, 1] the target: 58\nWhen input is [43, 1, 53, 44, 1, 58] the target: 56\nWhen input is [43, 1, 53, 44, 1, 58, 56] the target: 59\nWhen input is [43, 1, 53, 44, 1, 58, 56, 59] the target: 43\nWhen input is [43] the target: 52\nWhen input is [43, 52] the target: 1\nWhen input is [43, 52, 1] the target: 44\nWhen input is [43, 52, 1, 44] the target: 56\nWhen input is [43, 52, 1, 44, 56] the target: 53\nWhen input is [43, 52, 1, 44, 56, 53] the target: 51\nWhen input is [43, 52, 1, 44, 56, 53, 51] the target: 1\nWhen input is [43, 52, 1, 44, 56, 53, 51, 1] the target: 39\nWhen input is [11] the target: 1\nWhen input is [11, 1] the target: 47\nWhen input is [11, 1, 47] the target: 58\nWhen input is [11, 1, 47, 58] the target: 1\nWhen input is [11, 1, 47, 58, 1] the target: 47\nWhen input is [11, 1, 47, 58, 1, 47] the target: 57\nWhen input is [11, 1, 47, 58, 1, 47, 57] the target: 1\nWhen input is [11, 1, 47, 58, 1, 47, 57, 1] the target: 39\nWhen input is [56] the target: 47\nWhen input is [56, 47] the target: 58\nWhen input is [56, 47, 58] the target: 1\nWhen input is [56, 47, 58, 1] the target: 61\nWhen input is [56, 47, 58, 1, 61] the target: 43\nWhen input is [56, 47, 58, 1, 61, 43] the target: 56\nWhen input is [56, 47, 58, 1, 61, 43, 56] the target: 43\nWhen input is [56, 47, 58, 1, 61, 43, 56, 43] the target: 1\nWhen input is [47] the target: 43\nWhen input is [47, 43] the target: 58\nWhen input is [47, 43, 58] the target: 6\nWhen input is [47, 43, 58, 6] the target: 1\nWhen input is [47, 43, 58, 6, 1] the target: 53\nWhen input is [47, 43, 58, 6, 1, 53] the target: 52\nWhen input is [47, 43, 58, 6, 1, 53, 52] the target: 1\nWhen input is [47, 43, 58, 6, 1, 53, 52, 1] the target: 32\nWhen input is [40] the target: 56\nWhen input is [40, 56] the target: 53\nWhen input is [40, 56, 53] the target: 58\nWhen input is [40, 56, 53, 58] the target: 46\nWhen input is [40, 56, 53, 58, 46] the target: 43\nWhen input is [40, 56, 53, 58, 46, 43] the target: 56\nWhen input is [40, 56, 53, 58, 46, 43, 56] the target: 1\nWhen input is [40, 56, 53, 58, 46, 43, 56, 1] the target: 47\nWhen input is [59] the target: 56\nWhen input is [59, 56] the target: 1\nWhen input is [59, 56, 1] the target: 53\nWhen input is [59, 56, 1, 53] the target: 44\nWhen input is [59, 56, 1, 53, 44] the target: 1\nWhen input is [59, 56, 1, 53, 44, 1] the target: 58\nWhen input is [59, 56, 1, 53, 44, 1, 58] the target: 46\nWhen input is [59, 56, 1, 53, 44, 1, 58, 46] the target: 43\nWhen input is [13] the target: 52\nWhen input is [13, 52] the target: 42\nWhen input is [13, 52, 42] the target: 1\nWhen input is [13, 52, 42, 1] the target: 57\nWhen input is [13, 52, 42, 1, 57] the target: 39\nWhen input is [13, 52, 42, 1, 57, 39] the target: 47\nWhen input is [13, 52, 42, 1, 57, 39, 47] the target: 42\nWhen input is [13, 52, 42, 1, 57, 39, 47, 42] the target: 1\nWhen input is [45] the target: 39\nWhen input is [45, 39] the target: 56\nWhen input is [45, 39, 56] the target: 50\nWhen input is [45, 39, 56, 50] the target: 39\nWhen input is [45, 39, 56, 50, 39] the target: 52\nWhen input is [45, 39, 56, 50, 39, 52] the target: 42\nWhen input is [45, 39, 56, 50, 39, 52, 42] the target: 8\nWhen input is [45, 39, 56, 50, 39, 52, 42, 8] the target: 1\nWhen input is [43] the target: 39\nWhen input is [43, 39] the target: 56\nWhen input is [43, 39, 56] the target: 57\nWhen input is [43, 39, 56, 57] the target: 6\nWhen input is [43, 39, 56, 57, 6] the target: 0\nWhen input is [43, 39, 56, 57, 6, 0] the target: 13\nWhen input is [43, 39, 56, 57, 6, 0, 13] the target: 52\nWhen input is [43, 39, 56, 57, 6, 0, 13, 52] the target: 42\nWhen input is [8] the target: 0\nWhen input is [8, 0] the target: 0\nWhen input is [8, 0, 0] the target: 17\nWhen input is [8, 0, 0, 17] the target: 16\nWhen input is [8, 0, 0, 17, 16] the target: 35\nWhen input is [8, 0, 0, 17, 16, 35] the target: 13\nWhen input is [8, 0, 0, 17, 16, 35, 13] the target: 30\nWhen input is [8, 0, 0, 17, 16, 35, 13, 30] the target: 16\nWhen input is [47] the target: 57\nWhen input is [47, 57] the target: 1\nWhen input is [47, 57, 1] the target: 58\nWhen input is [47, 57, 1, 58] the target: 46\nWhen input is [47, 57, 1, 58, 46] the target: 43\nWhen input is [47, 57, 1, 58, 46, 43] the target: 1\nWhen input is [47, 57, 1, 58, 46, 43, 1] the target: 61\nWhen input is [47, 57, 1, 58, 46, 43, 1, 61] the target: 47\nWhen input is [1] the target: 52\nWhen input is [1, 52] the target: 47\nWhen input is [1, 52, 47] the target: 43\nWhen input is [1, 52, 47, 43] the target: 41\nWhen input is [1, 52, 47, 43, 41] the target: 43\nWhen input is [1, 52, 47, 43, 41, 43] the target: 1\nWhen input is [1, 52, 47, 43, 41, 43, 1] the target: 28\nWhen input is [1, 52, 47, 43, 41, 43, 1, 28] the target: 50\nWhen input is [59] the target: 54\nWhen input is [59, 54] the target: 1\nWhen input is [59, 54, 1] the target: 58\nWhen input is [59, 54, 1, 58] the target: 46\nWhen input is [59, 54, 1, 58, 46] the target: 43\nWhen input is [59, 54, 1, 58, 46, 43] the target: 1\nWhen input is [59, 54, 1, 58, 46, 43, 1] the target: 63\nWhen input is [59, 54, 1, 58, 46, 43, 1, 63] the target: 43\nWhen input is [47] the target: 58\nWhen input is [47, 58] the target: 1\nWhen input is [47, 58, 1] the target: 57\nWhen input is [47, 58, 1, 57] the target: 43\nWhen input is [47, 58, 1, 57, 43] the target: 43\nWhen input is [47, 58, 1, 57, 43, 43] the target: 51\nWhen input is [47, 58, 1, 57, 43, 43, 51] the target: 57\nWhen input is [47, 58, 1, 57, 43, 43, 51, 57] the target: 1\nWhen input is [43] the target: 50\nWhen input is [43, 50] the target: 1\nWhen input is [43, 50, 1] the target: 51\nWhen input is [43, 50, 1, 51] the target: 43\nWhen input is [43, 50, 1, 51, 43] the target: 1\nWhen input is [43, 50, 1, 51, 43, 1] the target: 58\nWhen input is [43, 50, 1, 51, 43, 1, 58] the target: 53\nWhen input is [43, 50, 1, 51, 43, 1, 58, 53] the target: 1\nWhen input is [39] the target: 50\nWhen input is [39, 50] the target: 50\nWhen input is [39, 50, 50] the target: 1\nWhen input is [39, 50, 50, 1] the target: 42\nWhen input is [39, 50, 50, 1, 42] the target: 43\nWhen input is [39, 50, 50, 1, 42, 43] the target: 57\nWhen input is [39, 50, 50, 1, 42, 43, 57] the target: 54\nWhen input is [39, 50, 50, 1, 42, 43, 57, 54] the target: 39\nWhen input is [1] the target: 54\nWhen input is [1, 54] the target: 56\nWhen input is [1, 54, 56] the target: 39\nWhen input is [1, 54, 56, 39] the target: 63\nWhen input is [1, 54, 56, 39, 63] the target: 1\nWhen input is [1, 54, 56, 39, 63, 1] the target: 63\nWhen input is [1, 54, 56, 39, 63, 1, 63] the target: 53\nWhen input is [1, 54, 56, 39, 63, 1, 63, 53] the target: 59\nWhen input is [43] the target: 57\nWhen input is [43, 57] the target: 58\nWhen input is [43, 57, 58] the target: 6\nWhen input is [43, 57, 58, 6] the target: 1\nWhen input is [43, 57, 58, 6, 1] the target: 53\nWhen input is [43, 57, 58, 6, 1, 53] the target: 44\nWhen input is [43, 57, 58, 6, 1, 53, 44] the target: 1\nWhen input is [43, 57, 58, 6, 1, 53, 44, 1] the target: 61\nWhen input is [53] the target: 6\nWhen input is [53, 6] the target: 1\nWhen input is [53, 6, 1] the target: 61\nWhen input is [53, 6, 1, 61] the target: 46\nWhen input is [53, 6, 1, 61, 46] the target: 43\nWhen input is [53, 6, 1, 61, 46, 43] the target: 52\nWhen input is [53, 6, 1, 61, 46, 43, 52] the target: 1\nWhen input is [53, 6, 1, 61, 46, 43, 52, 1] the target: 58\nWhen input is [1] the target: 58\nWhen input is [1, 58] the target: 56\nWhen input is [1, 58, 56] the target: 59\nWhen input is [1, 58, 56, 59] the target: 43\nWhen input is [1, 58, 56, 59, 43] the target: 1\nWhen input is [1, 58, 56, 59, 43, 1] the target: 44\nWhen input is [1, 58, 56, 59, 43, 1, 44] the target: 43\nWhen input is [1, 58, 56, 59, 43, 1, 44, 43] the target: 39\nWhen input is [39] the target: 58\nWhen input is [39, 58] the target: 1\nWhen input is [39, 58, 1] the target: 21\nWhen input is [39, 58, 1, 21] the target: 1\nWhen input is [39, 58, 1, 21, 1] the target: 57\nWhen input is [39, 58, 1, 21, 1, 57] the target: 46\nWhen input is [39, 58, 1, 21, 1, 57, 46] the target: 39\nWhen input is [39, 58, 1, 21, 1, 57, 46, 39] the target: 50\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in [\"train\" , \"eval\"]:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X , Y = get_batch(split)\n            logits , loss = model(X , Y)\n            losses[k] = loss.item()\n\n        out[split] = losses.mean()\n\n    model.train()\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:17:59.741315Z","iopub.execute_input":"2024-11-22T18:17:59.741753Z","iopub.status.idle":"2024-11-22T18:17:59.748708Z","shell.execute_reply.started":"2024-11-22T18:17:59.741713Z","shell.execute_reply":"2024-11-22T18:17:59.747303Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class Head(nn.Module):\n    \"\"\" One head of self-attention\"\"\"\n\n    def __init__(self,head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd,head_size,bias=False)\n        self.query = nn.Linear(n_embd,head_size,bias=False)\n        self.value = nn.Linear(n_embd,head_size,bias=False)\n        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x) #(B,T,C)\n        q = self.query(x) #(B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,C) @ (B, C, T) --> (B,T,T)\n        wei = wei.masked_fill(self.tril[:T,:T] == 0 , float('-inf')) #(B,T,T)\n        wei = F.softmax(wei , dim = -1) # (B,T,T)\n        # perform the weighted aggregation of the values\n        v = self.value(x) #(B,T,C)\n        out = wei @ v #(B,T,T) @ (B,T,C) -> (B,T,C)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:17:59.865784Z","iopub.execute_input":"2024-11-22T18:17:59.86619Z","iopub.status.idle":"2024-11-22T18:17:59.874868Z","shell.execute_reply.started":"2024-11-22T18:17:59.866155Z","shell.execute_reply":"2024-11-22T18:17:59.873673Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_head , head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n\n    def forward(self,x):\n        return torch.cat([h(x) for h in self.heads], dim = -1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:17:59.997493Z","iopub.execute_input":"2024-11-22T18:17:59.997969Z","iopub.status.idle":"2024-11-22T18:18:00.004973Z","shell.execute_reply.started":"2024-11-22T18:17:59.99793Z","shell.execute_reply":"2024-11-22T18:18:00.003256Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed  by a non-linearity \"\"\"\n\n    def __init__(self,n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd,n_embd),\n            nn.ReLU(),\n        )\n\n    def forward(self,x):\n        return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:18:02.3181Z","iopub.execute_input":"2024-11-22T18:18:02.318518Z","iopub.status.idle":"2024-11-22T18:18:02.325821Z","shell.execute_reply.started":"2024-11-22T18:18:02.31848Z","shell.execute_reply":"2024-11-22T18:18:02.324322Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    \n    def __init__(self,n_embd , n_head):\n        # n_embd: embedding dimension , n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head,head_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self,x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:32:35.697056Z","iopub.execute_input":"2024-11-22T18:32:35.69747Z","iopub.status.idle":"2024-11-22T18:32:35.704675Z","shell.execute_reply.started":"2024-11-22T18:32:35.697432Z","shell.execute_reply":"2024-11-22T18:32:35.703444Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"class BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n        self.postion_embedding_table = nn.Embedding(block_size , n_embd)\n        self.blocks = nn.Sequential(\n            Block(n_embd,n_head=4),\n            Block(n_embd,n_head=4),\n            Block(n_embd,n_head=4),\n        )\n        # self.sa_heads = MultiHeadAttention(4,n_embd//4) # i.e. 4 heads of 8-dimensional self-attention\n        # self.ffwd = FeedForward(n_embd)\n        self.lm_head = nn.Linear(n_embd,vocab_size)\n        \n    def forward(self, idx , targets=None):\n        B , T = idx.shape\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) #(B , T , C)\n        pos_emb = self.postion_embedding_table(torch.arange(T,device=device)) #(T,C)\n        x = tok_emb + pos_emb # (B , T , C)\n        # x = self.sa_heads(x) # applying multiple head of self-attention. (B,T,C)\n        # x = self.ffwd(x) # (B,T,C)\n        x = self.blocks(x) # (B,T,C) \n        logits = self.lm_head(x) #(B , T , vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B , T , C = logits.shape\n            logits = logits.view(B * T , C)\n            targets = targets.view(B * T)\n            loss = F.cross_entropy(logits,targets)\n        return logits , loss\n\n    def generate(self,idx,max_new_tokens):\n        # ids is (B,T) array of indices in the current context\n        for _ in  range(max_new_tokens):\n            #crop idx to the last block_size tokens\n            idx_cond = idx[:,-block_size:]\n            # get the predictions\n            logits , loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:,-1,:] # becomes(B,C)\n            # apply softmax to get probabilites\n            probs = F.softmax(logits,dim =-1) # (B,C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs,num_samples=1) #(B,1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx,idx_next), dim = 1) # (B , T+1)\n        return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:32:36.06488Z","iopub.execute_input":"2024-11-22T18:32:36.06528Z","iopub.status.idle":"2024-11-22T18:32:36.075843Z","shell.execute_reply.started":"2024-11-22T18:32:36.065246Z","shell.execute_reply":"2024-11-22T18:32:36.074628Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"model = BigramLanguageModel()\nm = model.to(device)\n# logits , loss = m(xb,yb)\n# print(logits.shape)\n# print(loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:32:36.235435Z","iopub.execute_input":"2024-11-22T18:32:36.235894Z","iopub.status.idle":"2024-11-22T18:32:36.248197Z","shell.execute_reply.started":"2024-11-22T18:32:36.235857Z","shell.execute_reply":"2024-11-22T18:32:36.247Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# # create a PyTorch optimizer\n# optimizer = torch.optim.AdamW(m.parameters(),lr = 1e-3)\n\n# for steps in range(10000):\n\n#     # sample a batch of data\n#     xb , yb = get_batch(\"train\")\n\n#     # evaluate the loss\n#     logits , loss = m(xb,yb)\n#     optimizer.zero_grad(set_to_none=True)\n#     loss.backward()\n#     optimizer.step()\n\n# print(loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:32:36.421061Z","iopub.execute_input":"2024-11-22T18:32:36.421454Z","iopub.status.idle":"2024-11-22T18:32:36.426544Z","shell.execute_reply.started":"2024-11-22T18:32:36.421421Z","shell.execute_reply":"2024-11-22T18:32:36.425402Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(),lr = learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:32:36.726828Z","iopub.execute_input":"2024-11-22T18:32:36.72723Z","iopub.status.idle":"2024-11-22T18:32:36.732643Z","shell.execute_reply.started":"2024-11-22T18:32:36.727195Z","shell.execute_reply":"2024-11-22T18:32:36.731477Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"for iter in range(max_iters):\n    # every once in a while evalutate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f},val loss{losses['eval']:.4f}\")\n\n        # sample a batch of data\n        xb , yb = get_batch('train')\n\n        # evaluate the loss\n        logits , loss = m(xb , yb)\n        optimizer.zero_grad(set_to_none = True)\n        loss.backward()\n        optimizer.step()\n\n# generate from teh model\ncontext = torch.zeros((1,1),dtype = torch.long,device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:32:37.24474Z","iopub.execute_input":"2024-11-22T18:32:37.245112Z","iopub.status.idle":"2024-11-22T18:38:30.838927Z","shell.execute_reply.started":"2024-11-22T18:32:37.245081Z","shell.execute_reply":"2024-11-22T18:38:30.837599Z"}},"outputs":[{"name":"stdout","text":"step 0: train loss 4.1556,val loss4.1563\nstep 500: train loss 4.1051,val loss4.1067\nstep 1000: train loss 3.9878,val loss3.9952\nstep 1500: train loss 3.7516,val loss3.7682\nstep 2000: train loss 3.7749,val loss3.8367\nstep 2500: train loss 3.5196,val loss3.5518\nstep 3000: train loss 3.5042,val loss3.5315\nstep 3500: train loss 3.4672,val loss3.4942\nstep 4000: train loss 3.4103,val loss3.4325\nstep 4500: train loss 3.3914,val loss3.4563\nstep 5000: train loss 3.3958,val loss3.4318\nstep 5500: train loss 3.3685,val loss3.4394\nstep 6000: train loss 3.3532,val loss3.4077\nstep 6500: train loss 3.3603,val loss3.3893\nstep 7000: train loss 3.3455,val loss3.3854\nstep 7500: train loss 3.3489,val loss3.3995\nstep 8000: train loss 3.3461,val loss3.3865\nstep 8500: train loss 3.3522,val loss3.3776\nstep 9000: train loss 3.3413,val loss3.3804\nstep 9500: train loss 3.3475,val loss3.3747\nstep 10000: train loss 3.3351,val loss3.3775\nstep 10500: train loss 3.3385,val loss3.3747\nstep 11000: train loss 3.3286,val loss3.3567\nstep 11500: train loss 3.3153,val loss3.3461\nstep 12000: train loss 3.3211,val loss3.3434\nstep 12500: train loss 3.3281,val loss3.3592\nstep 13000: train loss 3.3090,val loss3.3537\nstep 13500: train loss 3.3172,val loss3.3396\nstep 14000: train loss 3.3114,val loss3.3315\nstep 14500: train loss 3.2998,val loss3.3429\nstep 15000: train loss 3.3049,val loss3.3277\nstep 15500: train loss 3.3139,val loss3.3313\nstep 16000: train loss 3.3060,val loss3.3461\nstep 16500: train loss 3.2896,val loss3.3226\nstep 17000: train loss 3.2926,val loss3.3230\nstep 17500: train loss 3.2966,val loss3.3269\nstep 18000: train loss 3.3012,val loss3.3282\nstep 18500: train loss 3.2933,val loss3.3069\nstep 19000: train loss 3.3057,val loss3.3053\nstep 19500: train loss 3.2901,val loss3.3186\nstep 20000: train loss 3.2830,val loss3.3152\nstep 20500: train loss 3.2745,val loss3.3131\nstep 21000: train loss 3.2762,val loss3.3024\nstep 21500: train loss 3.2822,val loss3.3129\nstep 22000: train loss 3.2901,val loss3.2971\nstep 22500: train loss 3.2875,val loss3.3053\nstep 23000: train loss 3.2781,val loss3.3146\nstep 23500: train loss 3.2874,val loss3.3040\nstep 24000: train loss 3.2851,val loss3.2982\nstep 24500: train loss 3.2834,val loss3.3242\nstep 25000: train loss 3.2751,val loss3.3066\nstep 25500: train loss 3.2665,val loss3.3057\nstep 26000: train loss 3.2739,val loss3.3025\nstep 26500: train loss 3.2798,val loss3.3072\nstep 27000: train loss 3.2882,val loss3.3127\nstep 27500: train loss 3.2957,val loss3.3124\nstep 28000: train loss 3.2885,val loss3.3128\nstep 28500: train loss 3.2764,val loss3.3291\nstep 29000: train loss 3.2876,val loss3.3123\nstep 29500: train loss 3.2883,val loss3.2978\nstep 30000: train loss 3.2764,val loss3.3073\nstep 30500: train loss 3.2645,val loss3.2938\nstep 31000: train loss 3.2597,val loss3.2816\nstep 31500: train loss 3.2544,val loss3.2786\nstep 32000: train loss 3.2545,val loss3.2884\nstep 32500: train loss 3.2648,val loss3.2704\nstep 33000: train loss 3.2550,val loss3.2826\nstep 33500: train loss 3.2494,val loss3.2704\nstep 34000: train loss 3.2516,val loss3.2820\nstep 34500: train loss 3.2479,val loss3.2639\nstep 35000: train loss 3.2394,val loss3.2687\nstep 35500: train loss 3.2480,val loss3.2806\nstep 36000: train loss 3.2186,val loss3.2479\nstep 36500: train loss 3.2243,val loss3.2511\nstep 37000: train loss 3.2334,val loss3.2448\nstep 37500: train loss 3.2282,val loss3.2477\nstep 38000: train loss 3.2221,val loss3.2451\nstep 38500: train loss 3.2159,val loss3.2421\nstep 39000: train loss 3.2426,val loss3.2478\nstep 39500: train loss 3.2360,val loss3.2597\nstep 40000: train loss 3.2138,val loss3.2312\nstep 40500: train loss 3.2284,val loss3.2275\nstep 41000: train loss 3.2292,val loss3.2487\nstep 41500: train loss 3.2213,val loss3.2372\nstep 42000: train loss 3.2049,val loss3.2277\nstep 42500: train loss 3.2051,val loss3.2190\nstep 43000: train loss 3.2023,val loss3.2188\nstep 43500: train loss 3.2075,val loss3.2307\nstep 44000: train loss 3.2125,val loss3.2368\nstep 44500: train loss 3.2038,val loss3.2141\nstep 45000: train loss 3.1891,val loss3.2206\nstep 45500: train loss 3.1798,val loss3.2080\nstep 46000: train loss 3.1821,val loss3.1863\nstep 46500: train loss 3.1708,val loss3.1916\nstep 47000: train loss 3.1867,val loss3.1936\nstep 47500: train loss 3.1631,val loss3.1797\nstep 48000: train loss 3.1692,val loss3.1597\nstep 48500: train loss 3.1595,val loss3.1556\nstep 49000: train loss 3.1600,val loss3.1689\nstep 49500: train loss 3.1693,val loss3.1771\nstep 50000: train loss 3.1682,val loss3.1759\nstep 50500: train loss 3.1622,val loss3.1669\nstep 51000: train loss 3.1578,val loss3.1771\nstep 51500: train loss 3.1512,val loss3.1554\nstep 52000: train loss 3.1543,val loss3.1629\nstep 52500: train loss 3.1535,val loss3.1467\nstep 53000: train loss 3.1539,val loss3.1481\nstep 53500: train loss 3.1452,val loss3.1449\nstep 54000: train loss 3.1450,val loss3.1361\nstep 54500: train loss 3.1432,val loss3.1421\nstep 55000: train loss 3.1400,val loss3.1496\nstep 55500: train loss 3.1408,val loss3.1406\nstep 56000: train loss 3.1407,val loss3.1434\nstep 56500: train loss 3.1306,val loss3.1389\nstep 57000: train loss 3.1346,val loss3.1348\nstep 57500: train loss 3.1374,val loss3.1325\nstep 58000: train loss 3.1314,val loss3.1289\nstep 58500: train loss 3.1273,val loss3.1280\nstep 59000: train loss 3.1297,val loss3.1217\nstep 59500: train loss 3.1287,val loss3.1225\nstep 60000: train loss 3.1348,val loss3.1235\nstep 60500: train loss 3.1295,val loss3.1374\nstep 61000: train loss 3.1450,val loss3.1389\nstep 61500: train loss 3.1331,val loss3.1348\nstep 62000: train loss 3.1285,val loss3.1372\nstep 62500: train loss 3.1353,val loss3.1337\nstep 63000: train loss 3.1450,val loss3.1428\nstep 63500: train loss 3.1365,val loss3.1292\nstep 64000: train loss 3.1206,val loss3.1211\nstep 64500: train loss 3.1315,val loss3.1225\nstep 65000: train loss 3.1360,val loss3.1286\nstep 65500: train loss 3.1317,val loss3.1209\nstep 66000: train loss 3.1214,val loss3.1222\nstep 66500: train loss 3.1098,val loss3.1208\nstep 67000: train loss 3.1166,val loss3.1246\nstep 67500: train loss 3.1332,val loss3.1307\nstep 68000: train loss 3.1221,val loss3.1359\nstep 68500: train loss 3.1231,val loss3.1333\nstep 69000: train loss 3.1275,val loss3.1292\nstep 69500: train loss 3.1246,val loss3.1254\nstep 70000: train loss 3.1117,val loss3.1160\nstep 70500: train loss 3.1072,val loss3.1141\nstep 71000: train loss 3.1072,val loss3.1124\nstep 71500: train loss 3.1043,val loss3.1155\nstep 72000: train loss 3.1052,val loss3.1075\nstep 72500: train loss 3.1141,val loss3.1148\nstep 73000: train loss 3.1057,val loss3.1140\nstep 73500: train loss 3.1146,val loss3.1291\nstep 74000: train loss 3.1291,val loss3.1283\nstep 74500: train loss 3.1140,val loss3.1139\nstep 75000: train loss 3.1013,val loss3.1105\nstep 75500: train loss 3.1259,val loss3.1284\nstep 76000: train loss 3.1207,val loss3.1251\nstep 76500: train loss 3.1005,val loss3.1160\nstep 77000: train loss 3.0993,val loss3.1068\nstep 77500: train loss 3.1127,val loss3.1068\nstep 78000: train loss 3.1156,val loss3.1224\nstep 78500: train loss 3.1149,val loss3.1126\nstep 79000: train loss 3.1012,val loss3.1026\nstep 79500: train loss 3.0888,val loss3.1044\nstep 80000: train loss 3.1092,val loss3.1167\nstep 80500: train loss 3.0769,val loss3.0811\nstep 81000: train loss 3.0625,val loss3.0771\nstep 81500: train loss 3.0658,val loss3.0824\nstep 82000: train loss 3.0764,val loss3.0735\nstep 82500: train loss 3.0642,val loss3.0831\nstep 83000: train loss 3.0751,val loss3.0725\nstep 83500: train loss 3.0812,val loss3.0861\nstep 84000: train loss 3.0833,val loss3.0970\nstep 84500: train loss 3.0973,val loss3.1027\nstep 85000: train loss 3.0691,val loss3.0803\nstep 85500: train loss 3.0370,val loss3.0410\nstep 86000: train loss 3.0446,val loss3.0488\nstep 86500: train loss 3.0589,val loss3.0638\nstep 87000: train loss 3.0725,val loss3.0748\nstep 87500: train loss 3.0484,val loss3.0536\nstep 88000: train loss 3.0341,val loss3.0341\nstep 88500: train loss 3.0724,val loss3.0784\nstep 89000: train loss 3.0650,val loss3.0715\nstep 89500: train loss 3.0305,val loss3.0405\nstep 90000: train loss 3.0495,val loss3.0524\nstep 90500: train loss 3.0582,val loss3.0507\nstep 91000: train loss 3.0200,val loss3.0129\nstep 91500: train loss 3.0148,val loss3.0117\nstep 92000: train loss 3.0297,val loss3.0218\nstep 92500: train loss 3.0225,val loss3.0307\nstep 93000: train loss 3.0126,val loss3.0159\nstep 93500: train loss 3.0036,val loss3.0069\nstep 94000: train loss 2.9976,val loss2.9950\nstep 94500: train loss 2.9903,val loss2.9928\nstep 95000: train loss 2.9914,val loss2.9842\nstep 95500: train loss 3.0037,val loss2.9979\nstep 96000: train loss 2.9919,val loss3.0008\nstep 96500: train loss 3.0123,val loss2.9981\nstep 97000: train loss 3.0009,val loss2.9918\nstep 97500: train loss 2.9756,val loss2.9709\nstep 98000: train loss 2.9939,val loss2.9993\nstep 98500: train loss 3.0221,val loss3.0172\nstep 99000: train loss 3.0130,val loss2.9979\nstep 99500: train loss 2.9775,val loss2.9723\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"print(decode(m.generate(torch.zeros((1,1),dtype=torch.long),max_new_tokens=500)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T19:14:47.811097Z","iopub.execute_input":"2024-11-22T19:14:47.811575Z","iopub.status.idle":"2024-11-22T19:14:49.642072Z","shell.execute_reply.started":"2024-11-22T19:14:47.811499Z","shell.execute_reply":"2024-11-22T19:14:49.640949Z"}},"outputs":[{"name":"stdout","text":"\nWaitan,nit tenrp n\nC tetahhanndb nteloriu  nr\nOztt nw aD:\nEAygap ytonnum fbomeonnt.\nWnSDIFert,dit,utinn ngyisdnl tigo?AOU,scpew.\nPaeuptot notano adisrdll  e  r dynknednesrs ti cisut tkneri kg rdrrb s,ygnnp'my\nH\nU MetyeIRRBEI\np\n-AO\nYPDNfu::\n:YZDLILC:\nIYA EN\nN:U:UTOGSuoir nhoI\n\nNo g rr hooesygml:  c he nw,vlit.\nSWonsn.Ooe eow,.\nS  \nUte;kvot ,s ncegoen o'!\nsuvpunnlm ednw tyew m,usn d . AO\nRuti doerimaLAr  :KHA\noeetemy ':\ns:'tnrtheCtot , cht cp ?\nISIsbse\nComtthif yofmenons tyrs Tetd eto rtee oleg er\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"## The mathematical trick in self-attention","metadata":{}},{"cell_type":"code","source":"# consider the following toy example:\n\ntorch.manual_seed(1337)\n# batch , time , channels\nB , T , C = 4 , 8 , 2\nx = torch.randn(B,T,C)\nx.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:53:52.955807Z","iopub.execute_input":"2024-11-22T14:53:52.956301Z","iopub.status.idle":"2024-11-22T14:53:52.968152Z","shell.execute_reply.started":"2024-11-22T14:53:52.956246Z","shell.execute_reply":"2024-11-22T14:53:52.96707Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 2])"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\n\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] #(t,C)\n        xbow[b,t] = torch.mean(xprev , 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:53:54.100311Z","iopub.execute_input":"2024-11-22T14:53:54.101108Z","iopub.status.idle":"2024-11-22T14:53:54.109257Z","shell.execute_reply.started":"2024-11-22T14:53:54.101065Z","shell.execute_reply":"2024-11-22T14:53:54.108221Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"x[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:53:54.846871Z","iopub.execute_input":"2024-11-22T14:53:54.847322Z","iopub.status.idle":"2024-11-22T14:53:54.859224Z","shell.execute_reply.started":"2024-11-22T14:53:54.847253Z","shell.execute_reply":"2024-11-22T14:53:54.858109Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.1808, -0.0700],\n        [-0.3596, -0.9152],\n        [ 0.6258,  0.0255],\n        [ 0.9545,  0.0643],\n        [ 0.3612,  1.1679],\n        [-1.3499, -0.5102],\n        [ 0.2360, -0.2398],\n        [-0.9211,  1.5433]])"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"xbow[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:53:55.661625Z","iopub.execute_input":"2024-11-22T14:53:55.662575Z","iopub.status.idle":"2024-11-22T14:53:55.670248Z","shell.execute_reply.started":"2024-11-22T14:53:55.662535Z","shell.execute_reply":"2024-11-22T14:53:55.669142Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.1808, -0.0700],\n        [-0.0894, -0.4926],\n        [ 0.1490, -0.3199],\n        [ 0.3504, -0.2238],\n        [ 0.3525,  0.0545],\n        [ 0.0688, -0.0396],\n        [ 0.0927, -0.0682],\n        [-0.0341,  0.1332]])"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"torch.manual_seed(42)\na = torch.tril(torch.ones(3,3))\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint(\"--\")\nprint('c=')\nprint(c)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:53:56.318208Z","iopub.execute_input":"2024-11-22T14:53:56.31916Z","iopub.status.idle":"2024-11-22T14:53:56.332199Z","shell.execute_reply.started":"2024-11-22T14:53:56.319121Z","shell.execute_reply":"2024-11-22T14:53:56.331139Z"}},"outputs":[{"name":"stdout","text":"a=\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[ 2.,  7.],\n        [ 8., 11.],\n        [14., 16.]])\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}